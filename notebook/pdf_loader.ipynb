{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader,TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ecfc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_pdfs(pdf_directory):\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"Loaded {len(documents)} pages\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents = process_all_pdfs(\"../data/pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4356bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "#from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_all_acts():\n",
    "    docs = []\n",
    "\n",
    "    # Pharmacy Act (from PDF)\n",
    "    pharmacy_loader = PyPDFLoader(\"../data/pdf/pharmacy.pdf\")  # <-- adjust path if needed\n",
    "    pharmacy_docs = pharmacy_loader.load()\n",
    "    for d in pharmacy_docs:\n",
    "        d.metadata[\"act_name\"] = \"Pharmacy Act (Nepali)\"\n",
    "        d.metadata[\"source_file\"] = \"pharmacy.pdf\"\n",
    "    docs.extend(pharmacy_docs)\n",
    "\n",
    "    # Immunization Act (from PDF)\n",
    "    imun_loader = PyPDFLoader(\"../data/pdf/immunization.pdf\")  # <-- adjust path if needed\n",
    "    imun_docs = imun_loader.load()\n",
    "    for d in imun_docs:\n",
    "        d.metadata[\"act_name\"] = \"Immunization Act (Nepali)\"\n",
    "        d.metadata[\"source_file\"] = \"immunization.pdf\"\n",
    "    docs.extend(imun_docs)\n",
    "\n",
    "    print(f\"Loaded {len(docs)} documents (pages) from PDFs\")\n",
    "    return docs\n",
    "\n",
    "# Text splitting into chunks + adding section_number metadata\n",
    "def split_documents(documents, chunk_size=800, chunk_overlap=150):\n",
    "    # Try to split first on \"धारा\", then on newlines, etc.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\nधारा\",   # split at section headings if present\n",
    "            \"\\n\\n\",\n",
    "            \"\\n\",\n",
    "            \"। \",       # sentence end\n",
    "            \" \"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Optionally extract section/धारा number into metadata if present\n",
    "    for doc in split_docs:\n",
    "        text = doc.page_content\n",
    "        match = re.search(r\"धारा\\s*([०१२३४५६७८९0-9]+)\", text)\n",
    "        if match:\n",
    "            doc.metadata[\"section_number\"] = match.group(1)\n",
    "\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    if split_docs:\n",
    "        print(\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff7741",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = load_all_acts()\n",
    "chunked_docs = split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1707be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_nepali_text(text: str) -> str:\n",
    "    \n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "   \n",
    "    bad_chars = [\"\\uf0a7\", \"\\uf0b7\", \"\\uf0d8\", \"\\uf0e5\", \"\\uf022\"]  \n",
    "    for ch in bad_chars:\n",
    "        text = text.replace(ch, \" \")\n",
    "\n",
    "    \n",
    "    text = re.sub(r\"[^\\u0900-\\u097F\\s।,;:?!०-९0-9\\-–]\", \" \", text)\n",
    "\n",
    "    \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "for d in chunked_docs:\n",
    "    d.page_content = clean_nepali_text(d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ecae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07adde1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61982329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding and vectorstore db\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List,Dict,Any,Tuple,Optional\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c27bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class EmbeddingManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "        device: Optional[str] = None,\n",
    "        normalize_embeddings: bool = True,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Embedding manager for multilingual (including Nepali) legal text.\n",
    "\n",
    "        - model_name: sentence-transformers model ID\n",
    "        - device: \"cuda\", \"cpu\", or None (auto-detect)\n",
    "        - normalize_embeddings: if True, L2-normalize embeddings (good for cosine similarity)\n",
    "        - batch_size: how many texts to encode per batch\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.normalize_embeddings = normalize_embeddings\n",
    "        self.batch_size = batch_size\n",
    "        self.model: SentenceTransformer | None = None\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name} on {self.device}\")\n",
    "            self.model = SentenceTransformer(self.model_name, device=self.device)\n",
    "            dim = self.model.get_sentence_embedding_dimension()\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {dim}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(\n",
    "            texts,\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=self.normalize_embeddings,\n",
    "        )\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# create global instance\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844bb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import Any, List\n",
    "\n",
    "import chromadb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_name: str = \"pdf_documents_v2\",  # new name to avoid mixing old embeddings\n",
    "        persist_directory: str = \"../data/vector_store\",\n",
    "        reset: bool = False,  # if True, delete existing collection on init\n",
    "    ):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self.reset = reset\n",
    "        self._initialize_store()\n",
    "    \n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # Optionally drop old collection (if you are rebuilding from scratch)\n",
    "            if self.reset:\n",
    "                try:\n",
    "                    self.client.delete_collection(self.collection_name)\n",
    "                    print(f\"Deleted existing collection: {self.collection_name}\")\n",
    "                except Exception:\n",
    "                    # If it doesn't exist yet, ignore\n",
    "                    pass\n",
    "\n",
    "            # Use cosine distance since we normalized embeddings\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\n",
    "                    \"description\": \"PDF document embeddings for RAG (Nepali law)\",\n",
    "                    \"hnsw:space\": \"cosine\",  # important if you want cosine similarity\n",
    "                },\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            metadata = dict(getattr(doc, \"metadata\", {}))\n",
    "            metadata[\"doc_index\"] = i\n",
    "            metadata[\"content_length\"] = len(getattr(doc, \"page_content\", \"\"))\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            documents_text.append(getattr(doc, \"page_content\", \"\"))\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text,\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# create a fresh store; use reset=True once when rebuilding\n",
    "vectorstore = VectorStore(reset=True)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e21514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load your acts from PDFs\n",
    "raw_docs = load_all_acts()          # uses PyPDFLoader for pharmacy.pdf + immunization.pdf\n",
    "\n",
    "# 2) Split into chunks\n",
    "chunked_docs = split_documents(raw_docs)\n",
    "\n",
    "# 3) Embed chunks with your embedding model (could be nepali_legal_model)\n",
    "texts = [d.page_content for d in chunked_docs]\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# 4) Add to the vector store\n",
    "vectorstore.add_documents(chunked_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2926be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[doc.page_content for doc in chunks]\n",
    "texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load acts from PDFs\n",
    "raw_docs = load_all_acts()          \n",
    "chunked_docs = split_documents(raw_docs)\n",
    "\n",
    "# 2) Embed chunks with the new EmbeddingManager\n",
    "texts = [d.page_content for d in chunked_docs]\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# 3) Add to the new vector store\n",
    "vectorstore.add_documents(chunked_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c940248",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever = RAGRetriever(vectorstore, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d01915",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48928b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rag_retriever.retrieve(\n",
    "    \"नेपाल फार्मेसी परिषद् ऐन, २०५७ अनुसार फार्मेसी व्यावसाय गनन के–के व्यवस्था छन्?\",\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "for doc in results:\n",
    "    print(f\"\\nRank {doc['rank']}, distance={doc['distance']:.4f}\")\n",
    "    print(\"Source:\", doc[\"metadata\"].get(\"source_file\"), \"page:\", doc[\"metadata\"].get(\"page\"))\n",
    "    print(doc[\"content\"][:400])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef911584",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_simple(\"pharmacy council le ke vanxa yo ainma?\", rag_retriever, llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ebe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def contains_devanagari(text: str) -> bool:\n",
    "    \"\"\"Return True if any Devanagari characters are present.\"\"\"\n",
    "    return bool(re.search(r'[\\u0900-\\u097F]', text))\n",
    "\n",
    "\n",
    "def normalize_to_nepali(query: str, llm) -> str:\n",
    "    \"\"\"\n",
    "    If query is Romanized Nepali (Latin script), convert it to Nepali (Devanagari).\n",
    "    If it's already in Devanagari, or clearly English, return as-is.\n",
    "    \"\"\"\n",
    "    if contains_devanagari(query):\n",
    "        return query\n",
    "\n",
    "    prompt = f\"\"\"तलको इनपुट रोमन नेपाली (Latin script मा लेखिएको नेपाली) पनि हुन सक्छ,\n",
    "वा अरू भाषा (जस्तै English) पनि हुन सक्छ।\n",
    "\n",
    "तपाईंको काम:\n",
    "- यदि इनपुट स्पष्ट रूपमा रोमन नेपाली छ भने, त्यसलाई सही नेपाली (देवनागरी) मा रूपान्तरण गर्नुहोस्।\n",
    "- यदि इनपुट रोमन नेपाली होइन (जस्तै pure English प्रश्न) छ भने, त्यसलाई जस्ताको तस्तै फर्काउनुहोस्।\n",
    "- कुनै पनि व्याख्या, अगाडि/पछि extra शब्दहरू नलेख्नुहोस्।\n",
    "- केवल रूपान्तरण गरिएको वा original वाक्य मात्र आउटपुट गर्नुहोस्।\n",
    "\n",
    "केही उदाहरण:\n",
    "- \"immunization act kaile aayeko ?\" -> \"खोप ऐन, २०७२ कहिले आएको हो ?\"\n",
    "- \"pharmacy council le ke vanxa?\" -> \"फार्मेसी काउन्सिलले के भन्छ ?\"\n",
    "- \"yo ain kahile lagu bhayo?\" -> \"यो ऐन कहिले लागू भयो ?\"\n",
    "\n",
    "प्रयोगकर्ताको इनपुट:\n",
    "{query}\n",
    "\n",
    "आउटपुट (केवल वाक्य):\n",
    "\"\"\"\n",
    "    resp = llm.invoke(prompt)\n",
    "    return resp.content.strip()\n",
    "\n",
    "\n",
    "def choose_where(query: str, norm_query: str) -> dict | None:\n",
    "    \"\"\"Decide which PDF to search based on keywords.\"\"\"\n",
    "    text = (query + \" \" + norm_query).lower()\n",
    "    # Pharmacy related\n",
    "    if any(word in text for word in [\"pharmacy\", \"pharmasi\", \"फार्मेसी\"]):\n",
    "        return {\"source_file\": \"pharmacy.pdf\"}\n",
    "    # Immunization / खोप related\n",
    "    if any(word in text for word in [\"immunization\", \"khop\", \"खोप\", \"इम्युनाइजेशन\"]):\n",
    "        return {\"source_file\": \"immunization.pdf\"}\n",
    "    return None\n",
    "\n",
    "\n",
    "def rag_simple(query: str, retriever, llm, top_k: int = 6) -> str:\n",
    "    # 0) Normalize Roman Nepali -> Devanagari\n",
    "    norm_query = normalize_to_nepali(query, llm)\n",
    "    print(\"Original query:\", query)\n",
    "    print(\"Normalized query:\", norm_query)\n",
    "\n",
    "    where = choose_where(query, norm_query)\n",
    "    print(\"Using where filter:\", where)\n",
    "\n",
    "    # 1) First try: retrieve with normalized Nepali\n",
    "    results = retriever.retrieve(norm_query, top_k=top_k, where=where)\n",
    "\n",
    "    # 1b) Fallback: if no results, try original query\n",
    "    if not results:\n",
    "        print(\"No results with normalized query, trying original query...\")\n",
    "        results = retriever.retrieve(query, top_k=top_k, where=where)\n",
    "\n",
    "    if not results:\n",
    "        return \"सहित सन्दर्भ (context) फेला परेन, त्यसैले म जवाफ दिन सक्दिन।\"\n",
    "\n",
    "    # 2) Build context (truncate if too long)\n",
    "    max_chars = 3500\n",
    "    context_parts = []\n",
    "    current_len = 0\n",
    "    for doc in results:\n",
    "        text = doc[\"content\"]\n",
    "        if current_len + len(text) > max_chars:\n",
    "            break\n",
    "        context_parts.append(text)\n",
    "        current_len += len(text)\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # 3) Prompt for answer\n",
    "    prompt = f\"\"\"तपाईं नेपाली कानुन बुझ्ने कानुनी सहायक हुनुहुन्छ। तल दिइएको सन्दर्भ \n",
    "फार्मेसी र खोप सम्बन्धी नेपाली कानूनबाट लिइएको हो।\n",
    "\n",
    "नियम:\n",
    "- केवल सन्दर्भमा स्पष्ट रूपमा लेखिएको आधारमा मात्र जवाफ दिनुहोस्।\n",
    "- यदि सोधिएको कुरा सन्दर्भमा स्पष्ट रूपमा छैन भने, जवाफमा लेख्नुहोस्:\n",
    "  \"मलाई थाहा छैन। यो जानकारी दिइएको सन्दर्भमा छैन।\"\n",
    "- आफ्नै अनुमान नगर्नुहोस्, अन्य सामान्य ज्ञान प्रयोग नगर्नुहोस्।\n",
    "- सकेसम्म दफा नम्बर, परिच्छेद वा शीर्षकको नाम उल्लेख गर्नुहोस्।\n",
    "- जवाफ छोटो तर ठोस, कानुनी रूपमा ठीक र नेपाली भाषामा दिनुहोस्।\n",
    "\n",
    "सन्दर्भ:\n",
    "{context}\n",
    "\n",
    "प्रश्न (प्रयोगकर्ताको मूल इनपुट):\n",
    "{query}\n",
    "\n",
    "अन्तर्रूप (normalize) गरिएको प्रश्न:\n",
    "{norm_query}\n",
    "\n",
    "जवाफ नेपाली भाषामा:\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c963d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever = RAGRetriever(vectorstore, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701277f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever.retrieve(\"नेपालमा फार्मेसी सञ्चालन गर्न कानुनले के कस्ता प्रावधानहरू राखेको छ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4573487",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of vectors in collection:\", vectorstore.collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be4f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "peek = vectorstore.collection.peek(3)\n",
    "print(\"Keys:\", peek.keys())\n",
    "print(\"Sample documents:\\n\")\n",
    "\n",
    "for i, doc in enumerate(peek.get(\"documents\", [])):\n",
    "    print(f\"Item {i}:\")\n",
    "    print(doc[:400])  # first 400 characters\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df9e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rag_retriever.retrieve(\n",
    "    \"यस खोप ऐन, २०७२ अनुसार खोप कार्यक्रम के हो?\",\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "print(\"Type of results:\", type(results))\n",
    "print(\"Number of docs:\", len(results))\n",
    "\n",
    "for doc in results:\n",
    "    print(f\"\\nRank: {doc['rank']}, distance: {doc['distance']:.4f}\")\n",
    "    print(doc['content'][:400])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52098f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "class RAGRetriever:\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        where: Optional[dict] = None,  # <-- NEW PARAM\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top k: {top_k}, where: {where}\")\n",
    "\n",
    "        # 1) Generate embedding for query\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        retrieved_docs: List[Dict[str, Any]] = []\n",
    "\n",
    "        try:\n",
    "            # 2) Query the collection (pass where filter)\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k,\n",
    "                where=where,  # <-- USE where HERE\n",
    "            )\n",
    "            # print(\"Raw results from vector store:\", results)  # optional debug\n",
    "\n",
    "            # 3) Process results if there are any\n",
    "            if results and results.get(\"documents\") and results[\"documents\"][0]:\n",
    "                documents = results[\"documents\"][0]\n",
    "                metadatas = results[\"metadatas\"][0]\n",
    "                distances = results[\"distances\"][0]\n",
    "                ids = results[\"ids\"][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(\n",
    "                    zip(ids, documents, metadatas, distances)\n",
    "                ):\n",
    "                    retrieved_docs.append({\n",
    "                        \"id\": doc_id,\n",
    "                        \"content\": document,\n",
    "                        \"metadata\": metadata,\n",
    "                        \"distance\": distance,  # smaller = more similar\n",
    "                        \"rank\": i + 1,\n",
    "                    })\n",
    "\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "\n",
    "        # 4) Always return a list\n",
    "        return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf2ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever = RAGRetriever(vectorstore, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if groq_api_key is None:\n",
    "    raise ValueError(\"GROQ_API_KEY not set in environment or .env file\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=groq_api_key,\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "def rag_simple(query: str, retriever, llm, top_k: int = 6) -> str:\n",
    "    # 0) Normalize (Roman -> Devanagari)\n",
    "    norm_query = normalize_to_nepali(query, llm)\n",
    "    print(\"Normalized query:\", norm_query)\n",
    "\n",
    "    # Decide which file to search\n",
    "    where = choose_where(query, norm_query)\n",
    "    print(\"Using where filter:\", where)\n",
    "\n",
    "    # 1) Retrieve\n",
    "    results = retriever.retrieve(norm_query, top_k=top_k, where=where)\n",
    "\n",
    "    if not results:\n",
    "        return \"सहित सन्दर्भ (context) फेला परेन, त्यसैले म जवाफ दिन सक्दिन।\"\n",
    "\n",
    "    # 2) Build context\n",
    "    max_chars = 3500\n",
    "    context_parts = []\n",
    "    current_len = 0\n",
    "    for doc in results:\n",
    "        text = doc[\"content\"]\n",
    "        if current_len + len(text) > max_chars:\n",
    "            break\n",
    "        context_parts.append(text)\n",
    "        current_len += len(text)\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # 3) Prompt\n",
    "    prompt = f\"\"\"तपाईं नेपाली कानुन बुझ्ने कानुनी सहायक हुनुहुन्छ। तल दिइएको सन्दर्भ \n",
    "फार्मेसी र खोप सम्बन्धी नेपाली कानूनबाट लिइएको हो।\n",
    "\n",
    "नियम:\n",
    "- सोधिएको प्रश्नको सीधा जवाफ दिनुहोस्।\n",
    "- मुख्य शर्तहरू वा प्रावधानहरू बुँदागत (१, २, ३...) रूपमा सारांश दिनुहोस्।\n",
    "- अनावश्यक रूपमा एउटै वाक्य/वाक्यांश दोहोर्याउनु हुँदैन।\n",
    "- सन्दर्भबाट ठ्याक्कै उतार्नु परे पनि, छोटो अंश मात्र उतार्नुहोस्, लामो वाक्य वा अनुच्छेद नदोहोऱ्याउनुहोस्।\n",
    "- सकेसम्म ऐनको नाम र धारा/परिच्छेद नम्बर (metadata मा भए) उल्लेख गर्नुहोस्।\n",
    "- यदि सोधिएको कुरा सन्दर्भमा स्पष्ट रूपमा छैन भने, जवाफमा लेख्नुहोस्:\n",
    "  \"मलाई थाहा छैन। यो जानकारी दिइएको सन्दर्भमा छैन।\"\n",
    "- आफ्नो अनुमान नगर्नुहोस्, अन्य सामान्य ज्ञान प्रयोग नगर्नुहोस्।\n",
    "\n",
    "सन्दर्भ:\n",
    "{context}\n",
    "\n",
    "प्रश्न (प्रयोगकर्ताको मूल इनपुट):\n",
    "{query}\n",
    "\n",
    "अन्तर्रूप (normalize) गरिएको प्रश्न:\n",
    "{norm_query}\n",
    "\n",
    "जवाफ नेपाली भाषामा, छोटो र बुँदागत रूपमा:\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b5cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_where(query: str, norm_query: str) -> dict | None:\n",
    "    text = (query + \" \" + norm_query).lower()\n",
    "    # Pharmacy‑related\n",
    "    if any(word in text for word in [\"pharmacy\", \"pharmasi\", \"फार्मेसी\"]):\n",
    "        return {\"source_file\": \"pharmacy.pdf\"}\n",
    "    # Immunization / खोप‑related\n",
    "    if any(word in text for word in [\"immunization\", \"khop\", \"खोप\", \"इम्युनाइजेशन\"]):\n",
    "        return {\"source_file\": \"immunization.pdf\"}\n",
    "    # Otherwise search all docs\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2fa812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "key = os.getenv(\"GROQ_API_KEY\")\n",
    "print(\"Key present:\", key is not None)\n",
    "print(\"Key length:\", len(key) if key else None)\n",
    "print(\"Key starts with:\", key[:4] if key else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accfa276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "print(\"Using key repr in llm init:\", repr(groq_api_key))\n",
    "print(\"Starts with:\", groq_api_key[:8] if groq_api_key else None)\n",
    "print(\"Length:\", len(groq_api_key) if groq_api_key else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f0db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "models = client.models.list()\n",
    "for m in models.data:\n",
    "    print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def contains_devanagari(text: str) -> bool:\n",
    "    \"\"\"Return True if any Devanagari characters are present.\"\"\"\n",
    "    return bool(re.search(r'[\\u0900-\\u097F]', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62de607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_nepali(query: str, llm) -> str:\n",
    "    \"\"\"\n",
    "    If query is Romanized Nepali (Latin script), convert it to Nepali in Devanagari.\n",
    "    If it's already in Devanagari, return as-is.\n",
    "    \"\"\"\n",
    "    # If it already has Devanagari, don't touch it\n",
    "    if contains_devanagari(query):\n",
    "        return query\n",
    "\n",
    "    prompt = f\"\"\"You are a transliteration engine, not a chatbot.\n",
    "\n",
    "TASK:\n",
    "- Convert Romanized Nepali written in Latin script into correct Nepali in Devanagari.\n",
    "- Do NOT translate, rephrase, or change the meaning.\n",
    "- Do NOT guess a different question.\n",
    "- Keep all words; if you don't know how to transliterate a word, copy it as-is.\n",
    "- Preserve question structure (question marks etc.).\n",
    "- Output ONLY the converted sentence, no explanation.\n",
    "\n",
    "GOOD examples (do this):\n",
    "- \"pharmacy ain le ke vanxa?\" -> \"फार्मेसी ऐनले के भन्छ ?\"\n",
    "- \"pharmacy council le yo ainma ke vanxa?\" -> \"फार्मेसी काउन्सिलले यो ऐनमा के भन्छ ?\"\n",
    "- \"immunization act kaile aayeko ?\" -> \"इम्युनाइजेशन ऐन कहिले आएको ?\"\n",
    "- \"yo ain namane kehi karbahi hunxa?\" -> \"यो ऐन नमाने केहि कारबाही हुन्छ ?\"\n",
    "\n",
    "BAD examples (never do this):\n",
    "- Changing \"pharmacy ain le ke vanxa?\" into \n",
    "  \"फार्मेसी व्यवसाय सञ्चालन गर्न के–के शर्त चाहिन्छ?\"  ✗  (WRONG: different meaning)\n",
    "- Changing topic or inventing extra information.\n",
    "\n",
    "User input:\n",
    "{query}\n",
    "\n",
    "Output (only the transliterated Nepali sentence):\n",
    "\"\"\"\n",
    "\n",
    "    resp = llm.invoke(prompt)\n",
    "    normalized = resp.content.strip()\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd1c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def rag_chat(message, chat_history):\n",
    "    \"\"\"\n",
    "    message: user input (string)\n",
    "    chat_history: list of (user, assistant) pairs\n",
    "    \"\"\"\n",
    "    # Call your RAG pipeline\n",
    "    answer = rag_simple(message, rag_retriever, llm)\n",
    "    # Append to history\n",
    "    chat_history = chat_history + [(message, answer)]\n",
    "    # Return: clear the textbox (\"\"), and updated history\n",
    "    return \"\", chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22adcfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_char_to_textbox(current_text: str, ch: str) -> str:\n",
    "    if current_text is None:\n",
    "        current_text = \"\"\n",
    "    return current_text + ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def rag_chat(message, history):\n",
    "    answer = rag_simple(message, rag_retriever, llm)\n",
    "    return answer\n",
    "\n",
    "def append_char_to_textbox(current_text: str, ch: str) -> str:\n",
    "    if current_text is None:\n",
    "        current_text = \"\"\n",
    "    return current_text + ch\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    # Your existing ChatInterface, unchanged\n",
    "    chat = gr.ChatInterface(\n",
    "        fn=rag_chat,\n",
    "        title=\"AI Lawyer – Nepali Health Law Assistant\",\n",
    "        description=(\n",
    "            \"फार्मेसी परिषद् ऐन, २०५७ र खोप ऐन, २०७२ को आधारमा कानुनी जानकारीका लागि।\\n\\n\"\n",
    "            \"> **Disclaimer:** यो केवल सूचना/शैक्षिक उद्देश्यको AI सहायक हो। \"\n",
    "            \"यो आधिकारिक कानुनी सल्लाह होइन। दर्ता भएका वकिलसँग परामर्श गर्नुहोस्।\"\n",
    "        ),\n",
    "        examples=[\n",
    "            [\"फार्मेसी व्यवसाय सञ्चालन गर्न के–के शर्त चाहिन्छ?\"],\n",
    "            [\"खोप ऐन, २०७२ कहिले लागू भयो?\"],\n",
    "            [\"pharmacy council le yo ainma ke vanxa?\"],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Nepali virtual keyboard under the chat\n",
    "    gr.Markdown(\"**नेपाली किबोर्ड (क्लिक गरेर टाइप गर्नुहोस्):**\")\n",
    "\n",
    "    row1 = [\"क\", \"ख\", \"ग\", \"घ\", \"ङ\", \"च\", \"छ\", \"ज\", \"झ\", \"ञ\"]\n",
    "    row2 = [\"ट\", \"ठ\", \"ड\", \"ढ\", \"ण\", \"त\", \"थ\", \"द\", \"ध\", \"न\"]\n",
    "    row3 = [\"प\", \"फ\", \"ब\", \"भ\", \"म\", \"य\", \"र\", \"ल\", \"व\"]\n",
    "    row4 = [\"श\", \"ष\", \"स\", \"ह\", \"ा\", \"ि\", \"ी\", \"ु\", \"ू\", \"ृ\"]\n",
    "    special_keys = [\n",
    "        (\"Space\", \" \"),   \n",
    "        (\"?\", \"?\"),       \n",
    "        (\"।\", \"।\"),       \n",
    "        (\",\", \",\"),       \n",
    "    ]\n",
    "\n",
    "    def make_append_fn(ch):\n",
    "        def fn(current_text):\n",
    "            return append_char_to_textbox(current_text, ch)\n",
    "        return fn\n",
    "\n",
    "    with gr.Row():\n",
    "        for ch in row1:\n",
    "            btn = gr.Button(ch)\n",
    "            btn.click(make_append_fn(ch), inputs=chat.textbox, outputs=chat.textbox)\n",
    "\n",
    "    with gr.Row():\n",
    "        for ch in row2:\n",
    "            btn = gr.Button(ch)\n",
    "            btn.click(make_append_fn(ch), inputs=chat.textbox, outputs=chat.textbox)\n",
    "\n",
    "    with gr.Row():\n",
    "        for ch in row3:\n",
    "            btn = gr.Button(ch)\n",
    "            btn.click(make_append_fn(ch), inputs=chat.textbox, outputs=chat.textbox)\n",
    "\n",
    "    with gr.Row():\n",
    "        for ch in row4:\n",
    "            btn = gr.Button(ch)\n",
    "            btn.click(make_append_fn(ch), inputs=chat.textbox, outputs=chat.textbox)\n",
    "    with gr.Row():\n",
    "        for label, ch in special_keys:\n",
    "            btn = gr.Button(label)\n",
    "            btn.click(make_append_fn(ch), inputs=chat.textbox, outputs=chat.textbox)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4928fd52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ytrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
